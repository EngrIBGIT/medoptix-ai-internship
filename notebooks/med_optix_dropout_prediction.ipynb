{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install boto3\n",
    "%pip install psycopg2\n",
    "%pip install sqlalchemy\n",
    "%pip install dotenv\n",
    "\n",
    "\n",
    "%pip matplotlib\n",
    "%pip plotly\n",
    "%pip seaborn \n",
    "\n",
    "\n",
    "# Run this cell to install these packages\n",
    "# !pip install pandas boto3 sqlalchemy python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "\n",
    "\n",
    "\n",
    "#pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "#Prediction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import shap\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Imports and Setup ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay,\n",
    "    accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadcf482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ea2e9",
   "metadata": {},
   "source": [
    "# <h2 align=\"center\">Data Input</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c510ed6",
   "metadata": {},
   "source": [
    "### Define a Function to Query & Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5408b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to fetch data from the database\n",
    "def get_db_connection():\n",
    "    db_url = (\n",
    "        f\"postgresql://{os.environ['DB_USER']}:{os.environ['DB_PASSWORD']}@\"\n",
    "        f\"{os.environ['DB_HOST']}:{os.environ['DB_PORT']}/{os.environ['DB_NAME']}\"\n",
    "    )\n",
    "    return create_engine(db_url)\n",
    "\n",
    "# Instantiate the database connection\n",
    "engine = get_db_connection()\n",
    "\n",
    "\n",
    "# Define query to fetch data from each table\n",
    "query_clinics = \"SELECT * FROM clinics;\"\n",
    "query_patients = \"SELECT * FROM patients;\"\n",
    "query_sessions = \"SELECT * FROM sessions;\"\n",
    "query_feedback = \"SELECT * FROM feedback;\"\n",
    "query_dropout_flags = \"SELECT * FROM dropout_flags;\"\n",
    "query_interventions = \"SELECT * FROM interventions;\"\n",
    "\n",
    "# Load data from each table into a DataFrame\n",
    "clinics_df = pd.read_sql(query_clinics, engine)\n",
    "patients_df = pd.read_sql(query_patients, engine)\n",
    "sessions_df = pd.read_sql(query_sessions, engine)\n",
    "feedback_df = pd.read_sql(query_feedback, engine)\n",
    "dropout_flags_df = pd.read_sql(query_dropout_flags, engine)\n",
    "interventions_df = pd.read_sql(query_interventions, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b125d28",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- Create New features\n",
    "- Merge relevant datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f7692",
   "metadata": {},
   "source": [
    "## NOTE: \n",
    "After the EDA, the data is pretty messy, hence we will:\n",
    "- define the `patients data and session data` and merge the data together followed by, \n",
    "- creating a pipeline (define numerical and categorical columns),\n",
    "- Then preprocessing the data before doing the clustering,\n",
    "- Then do the clustering,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b274932",
   "metadata": {},
   "source": [
    "## Patient Segmentation:\n",
    "To see which cluster each patient belong to, to see the behaviour of how each patient behave in each cluster\n",
    "\n",
    "### Steps:\n",
    "    - Define the patients(who they are i.e the features)\n",
    "    - Define patients sessions\n",
    "\n",
    "- Defining patients/sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f996c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinics_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00981a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1a2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d56ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2567b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_flags_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a8224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interventions_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c259e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = patients_df.select_dtypes(include='number').corr()\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix â€” Numeric Features Only', fontsize=16, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c5035f",
   "metadata": {},
   "source": [
    "#### Augment the `patient_session_df` with more datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5704fe5",
   "metadata": {},
   "source": [
    "# Feature Engineering: Merge relevant features ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c9bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate session features per patient\n",
    "session_agg = sessions_df.groupby(\"patient_id\").agg({\n",
    "    \"duration\": \"mean\",\n",
    "    \"pain_level\": \"mean\",\n",
    "    \"home_adherence_pc\": \"mean\",\n",
    "    \"satisfaction\": \"mean\"\n",
    "}).rename(columns={\n",
    "    \"duration\": \"avg_duration\",\n",
    "    \"pain_level\": \"avg_pain_level\",\n",
    "    \"home_adherence_pc\": \"avg_home_adherence\",\n",
    "    \"satisfaction\": \"avg_satisfaction\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ec495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate feedback sentiment per patient\n",
    "feedback_sessions = feedback_df.merge(sessions_df[[\"session_id\", \"patient_id\"]], on=\"session_id\", how=\"left\")\n",
    "feedback_agg = feedback_sessions.groupby(\"patient_id\").agg({\n",
    "    \"sentiment\": \"mean\"\n",
    "}).rename(columns={\"sentiment\": \"avg_sentiment\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57193b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate interventions per patient\n",
    "interventions_agg = interventions_df.groupby(\"patient_id\").agg({\n",
    "    \"responded\": \"mean\"\n",
    "}).rename(columns={\"responded\": \"intervention_response_rate\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1557aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge clinics info to patients\n",
    "patients_clinic = patients_df.merge(clinics_df, on=\"clinic_id\", how=\"left\", suffixes=('', '_clinic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ebb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all features\n",
    "patient_sel = patients_clinic.set_index(\"patient_id\").join([\n",
    "    session_agg, feedback_agg, interventions_agg\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af33f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dropout label\n",
    "label = dropout_flags_df.set_index(\"patient_id\").dropout\n",
    "patients_full = patient_sel.join(label, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4414400e",
   "metadata": {},
   "source": [
    " Feature Selection (RandomForest for importance) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648eaccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select candidate features (including engineered)\n",
    "candidate_features = [\n",
    "    \"age\", \"gender\", \"bmi\", \"smoker\", \"chronic_cond\", \"injury_type\", \"referral_source\", \"insurance_type\", \"consent\",\n",
    "    \"city\", \"country\", \"type\", \"postcode\", \"capacity\", \"staff_count\", \"speciality\", \"avg_rating\",  # clinic features\n",
    "    \"avg_duration\", \"avg_pain_level\", \"avg_home_adherence\", \"avg_satisfaction\",  # session agg\n",
    "    \"avg_sentiment\", \"intervention_response_rate\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5357b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features not present in the merged df\n",
    "candidate_features = [f for f in candidate_features if f in patients_full.columns]\n",
    "\n",
    "cat_cols = patients_full[candidate_features].select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = list(set(candidate_features) - set(cat_cols))\n",
    "target_col = \"dropout\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical for feature selection\n",
    "patients_fs = patients_full.copy()\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    patients_fs[col] = le.fit_transform(patients_fs[col].astype(str))\n",
    "le_target = LabelEncoder()\n",
    "y_fs = le_target.fit_transform(patients_fs[target_col])\n",
    "\n",
    "rf_fs = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_fs.fit(patients_fs[candidate_features], y_fs)\n",
    "importances = rf_fs.feature_importances_\n",
    "top_idx = np.argsort(importances)[::-1][:10]  # Select top 10 features\n",
    "selected_features = [candidate_features[i] for i in top_idx]\n",
    "print(\"Top features for dropout prediction:\", selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11879ff7",
   "metadata": {},
   "source": [
    "Prepare data for modeling ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dae9831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data for modeling ---\n",
    "X = patients_full[selected_features]\n",
    "y = patients_full[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.20, random_state=42)\n",
    "\n",
    "numerics = X.select_dtypes(include='number').columns.intersection(selected_features).tolist()\n",
    "categoricals = list(set(selected_features) - set(numerics))\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "categorical_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", numeric_pipe, numerics),\n",
    "    (\"cat\", categorical_pipe, categoricals)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f74bf64",
   "metadata": {},
   "source": [
    "Model Definitions and Hyperparameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definitions\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42),\n",
    "    \"Neural Net\": MLPClassifier(max_iter=500, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Grids\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model__C\": [0.1, 1, 10],\n",
    "        \"model__solver\": [\"lbfgs\", \"liblinear\"]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"model__n_estimators\": [100, 200],\n",
    "        \"model__max_depth\": [None, 5, 10]\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"model__max_depth\": [None, 5, 10],\n",
    "        \"model__min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"model__n_estimators\": [100, 200],\n",
    "        \"model__max_depth\": [3, 5, 7]\n",
    "    },\n",
    "    \"CatBoost\": {\n",
    "        \"model__iterations\": [100, 200],\n",
    "        \"model__depth\": [4, 6, 8]\n",
    "    },\n",
    "    \"Neural Net\": {\n",
    "        \"model__hidden_layer_sizes\": [(50,), (100,)],\n",
    "        \"model__alpha\": [0.0001, 0.001]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training & Hyperparameter Tuning\n",
    "from collections import OrderedDict\n",
    "\n",
    "results = OrderedDict()\n",
    "metrics = OrderedDict()\n",
    "best_params = OrderedDict()\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    pipe = ImbPipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    param_grid = param_grids.get(name, {})\n",
    "    grid = GridSearchCV(pipe, param_grid, scoring='f1', cv=3, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    results[name] = best_model\n",
    "    best_params[name] = grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training & Hyperparameter Tuning\n",
    "from collections import OrderedDict\n",
    "\n",
    "results = OrderedDict()\n",
    "metrics = OrderedDict()\n",
    "best_params = OrderedDict()\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    pipe = ImbPipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    param_grid = param_grids.get(name, {})\n",
    "    grid = GridSearchCV(pipe, param_grid, scoring='f1', cv=3, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    results[name] = best_model\n",
    "    best_params[name] = grid.best_params_\n",
    "\n",
    "    # Output best params and metrics immediately after training\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1] if hasattr(best_model, \"predict_proba\") else None\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, pos_label='Yes' if 'Yes' in np.unique(y) else 1)\n",
    "    roc = roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "    metrics[name] = {\"accuracy\": acc, \"f1\": f1, \"roc_auc\": roc}\n",
    "\n",
    "    print(\"Best Params:\", grid.best_params_)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    if roc is not None:\n",
    "        print(\"ROC AUC:\", roc)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43624895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "for name, best_model in results.items():\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "    plt.title(f\"{name} Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f4bf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models\n",
    "os.makedirs(\"models/saved_models\", exist_ok=True)\n",
    "\n",
    "for name, best_model in results.items():\n",
    "    model_path = f\"models/saved_models/{name.replace(' ', '_').lower()}_dropout_model.joblib\"\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"Saved model â†’ {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explanations\n",
    "for name, best_model in results.items():\n",
    "    try:\n",
    "        X_test_enc = best_model.named_steps[\"preprocessor\"].transform(X_test)\n",
    "        clf = best_model.named_steps[\"model\"]\n",
    "        explainer = shap.Explainer(clf, X_test_enc)\n",
    "        shap_values = explainer(X_test_enc)\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_test_enc, feature_names=best_model.named_steps[\"preprocessor\"].get_feature_names_out())\n",
    "        plt.title(f\"SHAP Summary for {name}\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP not supported for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"reports/figures\", exist_ok=True)\n",
    "\n",
    "for name, best_model in results.items():\n",
    "    try:\n",
    "        X_test_enc = best_model.named_steps[\"preprocessor\"].transform(X_test)\n",
    "        clf = best_model.named_steps[\"model\"]\n",
    "        explainer = shap.Explainer(clf, X_test_enc)\n",
    "        shap_values = explainer(X_test_enc)\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            X_test_enc,\n",
    "            feature_names=best_model.named_steps[\"preprocessor\"].get_feature_names_out(),\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f\"SHAP Summary for {name}\")\n",
    "        plt.savefig(f\"reports/figures/shap_{name.replace(' ', '_').lower()}.png\")\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP not supported for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289efb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Summary Table\n",
    "print(\"\\n=== Model Metrics Summary ===\")\n",
    "for name, m in metrics.items():\n",
    "    print(f\"{name}: Accuracy={m['accuracy']:.3f}, F1={m['f1']:.3f}, ROC_AUC={m['roc_auc']:.3f}\" if m['roc_auc'] is not None else f\"{name}: Accuracy={m['accuracy']:.3f}, F1={m['f1']:.3f}, ROC_AUC=N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07dc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
