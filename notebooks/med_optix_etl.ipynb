{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install boto3\n",
    "%pip install psycopg2\n",
    "%pip install sqlalchemy\n",
    "%pip install dotenv\n",
    "\n",
    "\n",
    "%pip matplotlib\n",
    "%pip plotly\n",
    "%pip seaborn \n",
    "\n",
    "\n",
    "# Run this cell to install these packages\n",
    "# !pip install pandas boto3 sqlalchemy python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1af5a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "\n",
    "#pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b68ce22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4357d5",
   "metadata": {},
   "source": [
    "# 1:  LOAD DATA INTO AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc0155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ibrah\\Documents\\Practicals\\Amdari\\medoptix-ai-internship\\medoptix_data\\raw\n"
     ]
    }
   ],
   "source": [
    "# Verify and print the absolute path to the raw data folder\n",
    "folder_data = os.path.abspath(\"./medoptix_data/raw\")\n",
    "print(folder_data)  # Verify the correct path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f307256e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './medoptix_data/raw' found successfully.\n"
     ]
    }
   ],
   "source": [
    "# Check if the folder exists\n",
    "folder_path = \"./medoptix_data/raw\"\n",
    "if not os.path.exists(folder_path):\n",
    "    print(f\"Error: Folder '{folder_path}' does not exist.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' found successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd87c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to upload to S3 (if needed)\n",
    "files = ['clinics.csv', 'dropout_flags.csv', 'feedback.csv', 'interventions.csv', 'patients.csv', 'sessions.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c5169c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 07:14:39,969 INFO:Loaded and cleaned clinics.csv with shape (8, 9)\n",
      "2025-07-09 07:14:40,004 INFO:Loaded and cleaned dropout_flags.csv with shape (81, 3)\n",
      "2025-07-09 07:14:40,679 INFO:Loaded and cleaned feedback.csv with shape (49165, 4)\n",
      "2025-07-09 07:14:40,749 INFO:Loaded and cleaned interventions.csv with shape (5016, 6)\n",
      "2025-07-09 07:14:40,904 INFO:Loaded and cleaned patients.csv with shape (1931, 12)\n",
      "2025-07-09 07:14:41,418 INFO:Loaded and cleaned sessions.csv with shape (70236, 10)\n"
     ]
    }
   ],
   "source": [
    "# Load and clean all CSVs from local directory\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
    "\n",
    "def clean_column_names(df):\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(' ', '_')\n",
    "        .str.replace('-', '_')\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df, subset=None):\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates(subset=subset)\n",
    "    after = len(df)\n",
    "    if before != after:\n",
    "        logging.info(f\"Removed {before - after} duplicates.\")\n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df, strategy='drop', fill_value=None):\n",
    "    if strategy == 'drop':\n",
    "        df = df.dropna()\n",
    "    elif strategy == 'mean':\n",
    "        df = df.fillna(df.mean(numeric_only=True))\n",
    "    elif strategy == 'median':\n",
    "        df = df.fillna(df.median(numeric_only=True))\n",
    "    elif strategy == 'fill':\n",
    "        df = df.fillna(fill_value)\n",
    "    return df\n",
    "\n",
    "def convert_types(df, type_map):\n",
    "    for col, dtype in type_map.items():\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].astype(dtype)\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Could not convert {col} to {dtype}: {e}\")\n",
    "    return df\n",
    "\n",
    "def validate_schema(df, required_columns, file_name=None):\n",
    "    missing = [col for col in required_columns if col not in df.columns]\n",
    "    if missing:\n",
    "        logging.warning(f\"{file_name or ''} missing columns: {missing}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def clean_dataframe(df, required_columns=None, type_map=None, na_strategy='drop', file_name=None):\n",
    "    df = clean_column_names(df)\n",
    "    schema_ok = True\n",
    "    if required_columns:\n",
    "        schema_ok = validate_schema(df, required_columns, file_name)\n",
    "    if schema_ok:\n",
    "        df = remove_duplicates(df)\n",
    "        df = handle_missing_values(df, strategy=na_strategy)\n",
    "        if type_map:\n",
    "            df = convert_types(df, type_map)\n",
    "    return df\n",
    "\n",
    "schemas = {\n",
    "    \"patients\": [\"patient_id\", \"age\", \"gender\", \"bmi\", \"smoker\", \"chronic_cond\", \"injury_type\", \"signup_date\", \"consent\", \"clinic_id\", \"referral_source\", \"insurance_type\"],\n",
    "    \"clinics\": [\"clinic_id\", \"city\", \"country\", \"type\", \"postcode\", \"capacity\", \"staff_count\", \"speciality\", \"avg_rating\"],\n",
    "    \"sessions\": [\"session_id\", \"patient_id\", \"date\", \"week\", \"duration\", \"pain_level\", \"exercise_type\", \"home_adherence_pc\", \"satisfaction\", \"therapist_id\"],\n",
    "    \"feedback\": [\"feedback_id\", \"session_id\", \"sentiment\", \"comments\"],\n",
    "    \"dropout_flags\": [\"patient_id\", \"dropout\", \"dropout_week\"],\n",
    "    \"interventions\": [\"intervention_id\", \"patient_id\", \"sent_at\", \"channel\", \"message\",\"responded\"]\n",
    "}\n",
    "type_maps = {\n",
    "    \"patients\": {\"age\": \"float\", \"bmi\": \"float\"},\n",
    "    \"sessions\": {\"duration\": \"float\", \"week\": \"int\", \"pain_level\": \"float\", \"home_adherence_pc\": \"float\", \"satisfaction\": \"float\"},\n",
    "    \"feedback\": {\"sentiment\": \"float\"},\n",
    "    \"dropout_flags\": {},\n",
    "    \"interventions\": {}\n",
    "}\n",
    "\n",
    "dfs = {}\n",
    "for file in files:\n",
    "    path = os.path.join(folder_data, file)\n",
    "    name = file.replace('.csv', '')\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        req_cols = schemas.get(name)\n",
    "        type_map = type_maps.get(name, {})\n",
    "        df = clean_dataframe(df, required_columns=req_cols, type_map=type_map, na_strategy='drop', file_name=file)\n",
    "        dfs[name] = df\n",
    "        logging.info(f\"Loaded and cleaned {file} with shape {df.shape}\")\n",
    "    else:\n",
    "        logging.warning(f\"{file} not found in {folder_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce725a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_file, table in table_mapping.items():\n",
    "    print(f\"\\n→ Processing {csv_file} → {table}\")\n",
    "    df = load_csv_from_s3(csv_file)  # or read from local\n",
    "    df_clean = clean_dataframe(\n",
    "        df,\n",
    "        required_columns=schemas[table],\n",
    "        na_strategy=\"drop\",\n",
    "        file_name=csv_file\n",
    "    )\n",
    "\n",
    "    if df_clean is not None:\n",
    "        os.makedirs(\"data/processed\", exist_ok=True)\n",
    "        processed_path = f\"data/processed/{csv_file}\"\n",
    "        df_clean.to_csv(processed_path, index=False)\n",
    "        print(f\" Saved cleaned '{csv_file}' to {processed_path}\")\n",
    "\n",
    "        insert_to_postgres(df_clean, table, engine)\n",
    "    else:\n",
    "        print(f\" Skipping '{csv_file}' due to cleaning/schema issues.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e20d1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-09 07:15:21,640 INFO:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded clinics.csv successfully\n",
      "uploaded dropout_flags.csv successfully\n",
      "uploaded feedback.csv successfully\n",
      "uploaded interventions.csv successfully\n",
      "uploaded patients.csv successfully\n",
      "uploaded sessions.csv successfully\n"
     ]
    }
   ],
   "source": [
    "# define variables (AKA parameters)\n",
    "bucket_name = \"amdari-demo-etl1\"\n",
    "folder_data = \"./medoptix_data/raw\"  # This is the local folder where the CSV files are stored\n",
    "target_folder = \"/medoptix/raw/\"\n",
    "\n",
    "# Activated a boto3 client (agent/interface) to connect to the S3 service\n",
    "# Make sure you have configured your AWS credentials before running this script\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "try:    \n",
    "    for filename in os.listdir(folder_data):   #looped into the data folder\n",
    "        if filename.endswith(\".csv\"):    #check for only files that ended with .csv\n",
    "            filepath = os.path.join(folder_data, filename) # define a path (eg /medoptix_data/patients.csv)\n",
    "            s3.upload_file(\n",
    "                Filename = filepath,                      #we used the client method (upload_file) to upload all the csv into our s3 bucket\n",
    "                Bucket= bucket_name,\n",
    "                Key = target_folder + filename   # combination of the path(s3) plus the filename (/medoptix/raw/patients.csv)\n",
    "            )\n",
    "            print(f\"uploaded {filename} successfully\")   # printed out the progress level\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91eea1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a client\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# defined our parameters\n",
    "bucket = \"amdari-demo-etl1\"\n",
    "prefix = \"/medoptix/raw/\"\n",
    "\n",
    "# List the files in the S3 bucket to confirm upload\n",
    "# List of files to upload to S3\n",
    "files = ['clinics.csv', 'dropout_flags.csv', 'feedback.csv', 'interventions.csv', 'patients.csv', 'sessions.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25c3d1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded clinics.csv successfully\n",
      "Downloaded dropout_flags.csv successfully\n",
      "Downloaded feedback.csv successfully\n",
      "Downloaded interventions.csv successfully\n",
      "Downloaded patients.csv successfully\n",
      "Downloaded sessions.csv successfully\n"
     ]
    }
   ],
   "source": [
    "# Download files from S3 bucket to local directory\n",
    "\n",
    "#looped through our desired files and dowload then from S3\n",
    "try:\n",
    "    for file in files:\n",
    "        s3.download_file(bucket, prefix + file, file)  ### medoptix/raw/patients.csv\n",
    "        print(f\"Downloaded {file} successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e58de",
   "metadata": {},
   "source": [
    "# 2: LOAD DATA FROM AWS S3 ------ POSTGRES DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c488fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the database connection parameters\n",
    "def get_db_connection():\n",
    "     db_url = (\n",
    "          f\"postgresql://{os.environ['DB_USER']}:{os.environ['DB_PASSWORD']}@\"\n",
    "            f\"{os.environ['DB_HOST']}:{os.environ['DB_PORT']}/{os.environ['DB_NAME']}\"\n",
    "     )\n",
    "\n",
    "     return create_engine(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de7091",
   "metadata": {},
   "source": [
    "## Upload From S3 Bucket to PostGres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fc4423",
   "metadata": {},
   "source": [
    "Import Libraries & Load Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82570fe6",
   "metadata": {},
   "source": [
    "### Set Up Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL configuration\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "\n",
    "# S3 configuration\n",
    "#AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "#AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "S3_BUCKET = \"amdari-demo-etl1\"\n",
    "S3_FOLDER = \"/medoptix/raw/\" # e.g., 'csv-data/',  This is the local folder where the CSV files are stored\n",
    "\n",
    "# SQLAlchemy engine setup\n",
    "engine = create_engine(f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194656e4",
   "metadata": {},
   "source": [
    "### Table Mapping & S3 Access Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map filenames to corresponding table names in your database\n",
    "table_mapping = {\n",
    "    \"clinics.csv\": \"clinics\",\n",
    "    \"patients.csv\": \"patients\",\n",
    "    \"sessions.csv\": \"sessions\",\n",
    "    \"feedback.csv\": \"feedback\",\n",
    "    \"dropout_flags.csv\": \"dropout_flags\",\n",
    "    \"interventions.csv\": \"interventions\" \n",
    "}\n",
    "\n",
    "# Initialize boto3 S3 client\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Function to load a CSV file from S3 into a pandas DataFrame\n",
    "def load_csv_from_s3(filename):\n",
    "    key = f\"{S3_FOLDER}{filename}\"\n",
    "    response = s3.get_object(Bucket=S3_BUCKET, Key=key)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    df = pd.read_csv(StringIO(content))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961bfc78",
   "metadata": {},
   "source": [
    "#### Insert the Files (Data) into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25befbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to insert a DataFrame into PostgreSQL\n",
    "def insert_to_postgres(df, table_name):\n",
    "    df.to_sql(table_name, engine, chunksize= 1000, method= \"multi\", index=False, if_exists='append')\n",
    "    print(f\"Inserted into: {table_name} ({len(df)} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb87f9",
   "metadata": {},
   "source": [
    "#### Execute the Full Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32beae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each CSV file: load from S3 and insert into DB\n",
    "for csv_file, table in table_mapping.items():\n",
    "    print(f\"\\n Processing {csv_file} → {table}\")\n",
    "    df = load_csv_from_s3(csv_file)\n",
    "    display(df.head())  # Preview top rows in notebook\n",
    "    insert_to_postgres(df, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns in DataFrame:\", df.columns.tolist())\n",
    "# Close the SQLAlchemy engine\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9ea2e9",
   "metadata": {},
   "source": [
    "# <h2 align=\"center\">Data Input</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c510ed6",
   "metadata": {},
   "source": [
    "### Define a Function to Query & Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5408b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to fetch data from the database\n",
    "def get_db_connection():\n",
    "    db_url = (\n",
    "        f\"postgresql://{os.environ['DB_USER']}:{os.environ['DB_PASSWORD']}@\"\n",
    "        f\"{os.environ['DB_HOST']}:{os.environ['DB_PORT']}/{os.environ['DB_NAME']}\"\n",
    "    )\n",
    "    return create_engine(db_url)\n",
    "\n",
    "# Instantiate the database connection\n",
    "engine = get_db_connection()\n",
    "\n",
    "\n",
    "# Define query to fetch data from each table\n",
    "query_clinics = \"SELECT * FROM clinics;\"\n",
    "query_patients = \"SELECT * FROM patients;\"\n",
    "query_sessions = \"SELECT * FROM sessions;\"\n",
    "query_feedback = \"SELECT * FROM feedback;\"\n",
    "query_dropout_flags = \"SELECT * FROM dropout_flags;\"\n",
    "query_interventions = \"SELECT * FROM interventions;\"\n",
    "\n",
    "# Load data from each table into a DataFrame\n",
    "clinics_df = pd.read_sql(query_clinics, engine)\n",
    "patients_df = pd.read_sql(query_patients, engine)\n",
    "sessions_df = pd.read_sql(query_sessions, engine)\n",
    "feedback_df = pd.read_sql(query_feedback, engine)\n",
    "dropout_flags_df = pd.read_sql(query_dropout_flags, engine)\n",
    "interventions_df = pd.read_sql(query_interventions, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb52a65",
   "metadata": {},
   "source": [
    "#### Display Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125877e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of each DataFrame\n",
    "clinics_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4ad9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8451e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d55b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_flags_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "interventions_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
